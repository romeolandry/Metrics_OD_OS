{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:167: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:179: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:183: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.2f')\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import colorsys\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input\n",
    "\n",
    "sess = K.get_session()\n",
    "\n",
    "\n",
    "dataDir= '/home/kamgo/Donnees/Master_projekt/Data/val2017 (1)/val2017'\n",
    "imageTpye = 'jpg'\n",
    "imageFiles ='%s/*.%s'%(dataDir,imageTpye)\n",
    "dataType='val2017'\n",
    "annFile = '%s/annotations/instances_%s.json'%(dataDir,dataType)\n",
    "annFile_result = '%s/annotations/instances_result_yolov3_O_%s.json'%(dataDir,dataType)\n",
    "\n",
    "model_path= 'model_data/yolov3_OT.h5'\n",
    "anchors_path= 'model_data/yolo_anchors.txt'\n",
    "classes_path= 'model_data/coco_classes.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox_image(image, size):\n",
    "    '''resize image with unchanged aspect ratio using padding'''\n",
    "    iw, ih = image.size\n",
    "    w, h = size\n",
    "    scale = min(w/iw, h/ih)\n",
    "    nw = int(iw*scale)\n",
    "    nh = int(ih*scale)\n",
    "\n",
    "    image = image.resize((nw,nh), Image.BICUBIC)\n",
    "    new_image = Image.new('RGB', size, (128,128,128))\n",
    "    new_image.paste(image, ((w-nw)//2, (h-nh)//2))\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    model_image_size = (416, 416)\n",
    "    boxed_image = letterbox_image(img, tuple(reversed(model_image_size)))\n",
    "    image_data = np.array(boxed_image, dtype='float32')\n",
    "    image_data /= 255.\n",
    "    #image_data = np.transpose(image_data, [2, 0, 1])\n",
    "    image_data = np.expand_dims(image_data, 0)\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_boxes_and_scores(feats, anchors, num_classes, input_shape, image_shape):\n",
    "    '''Process Conv layer output'''\n",
    "    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats,\n",
    "        anchors, num_classes, input_shape)\n",
    "    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n",
    "    boxes = K.reshape(boxes, [-1, 4])\n",
    "    box_scores = box_confidence * box_class_probs\n",
    "    box_scores = K.reshape(box_scores, [-1, num_classes])\n",
    "    return boxes, box_scores\n",
    "\n",
    "def yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape):\n",
    "    '''Get corrected boxes'''\n",
    "    box_yx = box_xy[..., ::-1]\n",
    "    box_hw = box_wh[..., ::-1]\n",
    "    input_shape = K.cast(input_shape, K.dtype(box_yx))\n",
    "    image_shape = K.cast(image_shape, K.dtype(box_yx))\n",
    "    new_shape = K.round(image_shape * K.min(input_shape/image_shape))\n",
    "    offset = (input_shape-new_shape)/2./input_shape\n",
    "    scale = input_shape/new_shape\n",
    "    box_yx = (box_yx - offset) * scale\n",
    "    box_hw *= scale\n",
    "\n",
    "    box_mins = box_yx - (box_hw / 2.)\n",
    "    box_maxes = box_yx + (box_hw / 2.)\n",
    "    boxes =  K.concatenate([\n",
    "        box_mins[..., 0:1],  # y_min\n",
    "        box_mins[..., 1:2],  # x_min\n",
    "        box_maxes[..., 0:1],  # y_max\n",
    "        box_maxes[..., 1:2]  # x_max\n",
    "    ])\n",
    "\n",
    "    # Scale boxes back to original image shape.\n",
    "    boxes *= K.concatenate([image_shape, image_shape])\n",
    "    return boxes\n",
    "\n",
    "    \n",
    "def yolo_boxes_and_scores(feats, anchors, num_classes, input_shape, image_shape):\n",
    "    '''Process Conv layer output'''\n",
    "    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats,\n",
    "        anchors, num_classes, input_shape)\n",
    "    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n",
    "    boxes = K.reshape(boxes, [-1, 4])\n",
    "    box_scores = box_confidence * box_class_probs\n",
    "    box_scores = K.reshape(box_scores, [-1, num_classes])\n",
    "    return boxes, box_scores\n",
    "\n",
    "    # Scale boxes back to original image shape.\n",
    "    boxes *= K.concatenate([image_shape, image_shape])\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_head(feats, anchors, num_classes, input_shape, calc_loss=False):\n",
    "    \"\"\"Convert final layer features to bounding box parameters.\"\"\"\n",
    "    num_anchors = len(anchors)\n",
    "    # Reshape to batch, height, width, num_anchors, box_params.\n",
    "    anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2])\n",
    "\n",
    "    grid_shape = K.shape(feats)[1:3] # height, width\n",
    "    grid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]),\n",
    "        [1, grid_shape[1], 1, 1])\n",
    "    grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]),\n",
    "        [grid_shape[0], 1, 1, 1])\n",
    "    grid = K.concatenate([grid_x, grid_y])\n",
    "    grid = K.cast(grid, K.dtype(feats))\n",
    "\n",
    "    feats = K.reshape(\n",
    "        feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5])\n",
    "\n",
    "    # Adjust preditions to each spatial grid point and anchor size.\n",
    "    box_xy = (K.sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[::-1], K.dtype(feats))\n",
    "    box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[::-1], K.dtype(feats))\n",
    "    box_confidence = K.sigmoid(feats[..., 4:5])\n",
    "    box_class_probs = K.sigmoid(feats[..., 5:])\n",
    "\n",
    "    if calc_loss == True:\n",
    "        return grid, feats, box_xy, box_wh\n",
    "    return box_xy, box_wh, box_confidence, box_class_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_anchors(anchors_path):\n",
    "        anchors_path = os.path.expanduser(anchors_path)\n",
    "        with open(anchors_path) as f:\n",
    "            anchors = f.readline()\n",
    "        anchors = [float(x) for x in anchors.split(',')]\n",
    "        return np.array(anchors).reshape(-1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_class(classes_path):\n",
    "        classes_path = os.path.expanduser(classes_path)\n",
    "        with open(classes_path) as f:\n",
    "            class_names = f.readlines()\n",
    "        class_names = [c.strip() for c in class_names]\n",
    "        return class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_eval(yolo_outputs,\n",
    "              anchors,\n",
    "              num_classes,\n",
    "              image_shape,\n",
    "              max_boxes=20,\n",
    "              score_threshold=.6,\n",
    "              iou_threshold=.5):\n",
    "    \"\"\"Evaluate YOLO model on given input and return filtered boxes.\"\"\"\n",
    "    num_layers = len(yolo_outputs)\n",
    "    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]] # default setting\n",
    "    input_shape = K.shape(yolo_outputs[0])[1:3] * 32\n",
    "    boxes = []\n",
    "    box_scores = []\n",
    "    for l in range(num_layers):\n",
    "        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l],\n",
    "            anchors[anchor_mask[l]], num_classes, input_shape, image_shape)\n",
    "        boxes.append(_boxes)\n",
    "        box_scores.append(_box_scores)\n",
    "    boxes = K.concatenate(boxes, axis=0)\n",
    "    box_scores = K.concatenate(box_scores, axis=0)\n",
    "\n",
    "    mask = box_scores >= score_threshold\n",
    "    max_boxes_tensor = K.constant(max_boxes, dtype='int32')\n",
    "    boxes_ = []\n",
    "    scores_ = []\n",
    "    classes_ = []\n",
    "    for c in range(num_classes):\n",
    "        # TODO: use keras backend instead of tf.\n",
    "        class_boxes = tf.boolean_mask(boxes, mask[:, c])\n",
    "        class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])\n",
    "        nms_index = tf.image.non_max_suppression(\n",
    "            class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)\n",
    "        class_boxes = K.gather(class_boxes, nms_index)\n",
    "        class_box_scores = K.gather(class_box_scores, nms_index)\n",
    "        classes = K.ones_like(class_box_scores, 'int32') * c\n",
    "        boxes_.append(class_boxes)\n",
    "        scores_.append(class_box_scores)\n",
    "        classes_.append(classes)\n",
    "    boxes_ = K.concatenate(boxes_, axis=0)\n",
    "    scores_ = K.concatenate(scores_, axis=0)\n",
    "    classes_ = K.concatenate(classes_, axis=0)\n",
    "\n",
    "    return boxes_, scores_, classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model_path,anchors_path,classes_path):\n",
    "        model_path = os.path.expanduser(model_path)\n",
    "        assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'\n",
    "\n",
    "        # Load model, or construct model and load weights.\n",
    "        num_anchors = len(_get_anchors(anchors_path))\n",
    "        num_classes = len(_get_class(classes_path))\n",
    "        \n",
    "        yolo_model = load_model(model_path, compile=False)\n",
    "        \n",
    "        print('{} model, anchors, and classes loaded.'.format(model_path))\n",
    "\n",
    "        # Generate output tensor targets for filtered bounding boxes.\n",
    "        input_image_shape = K.placeholder(shape=(2, ))\n",
    "        \n",
    "        boxes, scores, classes = yolo_eval(yolo_model.output, _get_anchors(anchors_path),len(_get_class(classes_path)), input_image_shape, score_threshold=0.6,iou_threshold=0.5)\n",
    "        \n",
    "        return yolo_model,boxes, scores, classes, input_image_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check diference between coco classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mapping_classes(coco_classes, path_to_instance_val):\n",
    "    val_json_categories = {}\n",
    "    instances_classes = []\n",
    "    mapping_list = []\n",
    "\n",
    "    with open(path_to_instance_val) as f:\n",
    "        d = json.load(f)\n",
    "        val_json_categories = d['categories']\n",
    "        \n",
    "    for category in val_json_categories:\n",
    "        instances_classes.append(category['name'])\n",
    "\n",
    "    for val1, val2 in zip(coco_classes, instances_classes):\n",
    "        mapping_list.append({val1:val2})\n",
    "\n",
    "    return mapping_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return matching for one classe\n",
    "def get_mapped(mapping_list, classe_name):\n",
    "\n",
    "    for map in mapping_list:\n",
    "        if(map.get(classe_name)):\n",
    "            return map.get(classe_name)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_result(path_to_instance_val,image_id,category_name,maping_classes):\n",
    "    \n",
    "    val_json_categories = {}\n",
    "    val_json_ann = {}\n",
    "    categoy_id = None\n",
    "    \n",
    "    category_name = get_mapped(maping_classes, category_name)\n",
    "    \n",
    "    with open(path_to_instance_val) as f:\n",
    "        d = json.load(f)\n",
    "        val_json_categories = d['categories']\n",
    "        val_json_ann = d['annotations']\n",
    "\n",
    "    for category in val_json_categories:\n",
    "        if (category['name']==category_name):\n",
    "            categoy_id = category['id']\n",
    "    return categoy_id      \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(path_to_file, result):\n",
    "    df = pd.DataFrame(result)\n",
    "    # print(df)\n",
    "    df.to_json(path_to_file,orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path_to_instance_val, path_to_images):\n",
    "    inferenced_instance_val_2017 = []\n",
    "    inferences_images = []    \n",
    "    val_json_images = {}\n",
    "    val_json_ann = {}\n",
    "    \n",
    "    with open(path_to_instance_val) as f:\n",
    "        d = json.load(f)\n",
    "        val_json_images = d['images']\n",
    "        val_json_ann = d['annotations']\n",
    "\n",
    "    for filename in glob.glob(imageFiles):\n",
    "        im=Image.open(filename) #os.path.basename(your_path)\n",
    "        #image_list.append({os.path.basename(filename):im})\n",
    "        image_name = os.path.basename(filename)\n",
    "        for image in val_json_images:\n",
    "            if(image['file_name'] == image_name):\n",
    "                #response = requests.get(image['coco_url'])\n",
    "                #img = Image.open(BytesIO(response.content))\n",
    "                inferences_images ={\"image_id\":image['id'],\"image\":im}    \n",
    "                yield inferences_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:507: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1797: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1940: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "model_data/yolov3_O.h5 model, anchors, and classes loaded.\n",
      "WARNING:tensorflow:From /home/kamgo/.virtualenvs/evaluation/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'font' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-367f550c2742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{} {:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mdraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mlabel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'font' is not defined"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "yolo_model,boxes, scores, classes, input_image_shape  = generate(model_path,anchors_path,classes_path)\n",
    "maping_classes = generate_mapping_classes(_get_class(classes_path), annFile)\n",
    "\n",
    "for inferences_images in get_data(annFile,imageFiles):\n",
    "    start = timer()\n",
    "    image = inferences_images['image']\n",
    "    image_data = preprocess(inferences_images['image'])\n",
    "\n",
    "\n",
    "    out_boxes, out_scores, out_classes = sess.run(\n",
    "        [boxes, scores, classes],\n",
    "        feed_dict={\n",
    "            yolo_model.input: image_data,\n",
    "            input_image_shape: [image.size[1], image.size[0]],\n",
    "            K.learning_phase(): 0\n",
    "        })    \n",
    "\n",
    "    for i, c in reversed(list(enumerate(out_classes))):\n",
    "\n",
    "        predicted_class = _get_class(classes_path)[c]\n",
    "        #print(predicted_class)\n",
    "        box = out_boxes[i]\n",
    "        #print(box) \n",
    "        score = out_scores[i]\n",
    "        #print(score)\n",
    "        category_id = gen_result(annFile,inferences_images['image_id'],predicted_class,maping_classes)\n",
    "        #print('predicted category: {}'.format(category_id))\n",
    "        \n",
    "        top, left, bottom, right = box\n",
    "        \n",
    "        # the coordinated is relatif to image in yolo\n",
    "        \n",
    "        top = max(0, np.floor(top + 0.5).astype('int32'))\n",
    "        left = max(0, np.floor(left + 0.5).astype('int32'))\n",
    "        bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
    "        right = min(image.size[0], np.floor(right + 0.5).astype('int32'))       \n",
    "        \n",
    "        bbox=[]\n",
    "        bbox.append(top)\n",
    "        bbox.append(left)\n",
    "        bbox.append(bottom)\n",
    "        bbox.append(right)\n",
    "        print({\"image_id\":inferences_images['image_id'],\"category_id\":category_id,\"bbox\":bbox,\"score\":score})\n",
    "        result.append({\"image_id\":inferences_images['image_id'],\"category_id\":category_id,\"bbox\":bbox,\"score\":score})    \n",
    "        #print(label, (left, top), (right, bottom))\n",
    "    end = timer()\n",
    "    print(end - start)\n",
    "save_json(annFile_result, result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
